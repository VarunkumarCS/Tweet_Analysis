{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrwHuwfbapPw"
   },
   "source": [
    "## Set up\n",
    "Let us get all the libaries initialized as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_FL--h5apPw"
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets\n",
    "pd.set_option('max_colwidth', 280)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ORGPFVgapPx"
   },
   "source": [
    "## Downloading Recent Tweets\n",
    "It is important to download the most recent tweets (especially if you are working as a group). Those who are working by themselves are allowed to use the downloaded files w/o setting up access to any twitter API (which can sometime be bit complicated). Twitter provides the API Tweepy (http://www.tweepy.org/) that makes it easy to access twitter content that is publicly available. We will also provide example code as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOOqNiaJapPy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Make sure you have set up tweepy if you are working locally.\n",
    "# https://www.pythoncentral.io/introduction-to-tweepy-twitter-for-python/\n",
    "# After set up, the following should run:\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSnwuTGlapPy"
   },
   "source": [
    "## PART 1:  Accessing Twitter API  (optional for individuals)\n",
    "In order to access Twitter API, you need to get keys by signing up as a Twitter developer. We will walk you through this process. \n",
    "* if you are working by yourself on this project, you can skip PART 1, and complete the project using the data files provided in the data folder. PART 1 is optional for those working by themselves. However, we highly recommend that you do Part 1 (after completing the project with offline data) if you would like to \"learn\" how to use Twitter API that might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ua3d1joapPz"
   },
   "source": [
    "### Task 1.1\n",
    "\n",
    "Follow the instructions below to get your Twitter API keys.  **Read the instructions completely before starting.**\n",
    "\n",
    "1. [Create a Twitter account](https://twitter.com/).  You can use an existing account if you have one; if you prefer to not do this assignment under your regular account, feel free to create a throw-away account.\n",
    "2. Under account settings, add your phone number to the account.\n",
    "3. [Create a Twitter developer account](https://developer.twitter.com/en/apply/) by clicking the 'Apply' button on the top right of the page. Attach it to your Twitter account. You'll have to fill out a form describing what you want to do with the developer account. Explain that you are doing this for a class at Rutgers University and that you don't know exactly what you're building yet and just need the account to get started. These applications are approved by some sort of AI system, so it doesn't matter exactly what you write. Just don't enter a bunch of alweiofalwiuhflawiuehflawuihflaiwhfe type stuff or you might get rejected.\n",
    "4. Once you're logged into your developer account, [create an application for this assignment](https://apps.twitter.com/app/new).  You can call it whatever you want, and you can write any URL when it asks for a web site.  You don't need to provide a callback URL.\n",
    "5. On the page for that application, find your Consumer Key and Consumer Secret.\n",
    "6. On the same page, create an Access Token.  Record the resulting Access Token and Access Token Secret.\n",
    "7. Edit the file [keys.json](keys.json) and replace the placeholders with your keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OgUJbFCapP0"
   },
   "source": [
    "## WARNING (Please Read) !!!!\n",
    "\n",
    "\n",
    "### Protect your Twitter Keys\n",
    "<span style=\"color:red\">\n",
    "If someone has your authentication keys, they can access your Twitter account and post as you!  So don't give them to anyone, and **don't write them down in this notebook**. \n",
    "</span>\n",
    "The usual way to store sensitive information like this is to put it in a separate file and read it programmatically.  That way, you can share the rest of your code without sharing your keys.  That's why we're asking you to put your keys in `keys.json` for this assignment.\n",
    "\n",
    "\n",
    "### Avoid making too many API calls.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Twitter limits developers to a certain rate of requests for data.  If you make too many requests in a short period of time, you'll have to wait awhile (around 15 minutes) before you can make more.  </span> \n",
    "So carefully follow the code examples you see and don't rerun cells without thinking.  Instead, always save the data you've collected to a file.  We've provided templates to help you do that.\n",
    "\n",
    "\n",
    "### Be careful about which functions you call!\n",
    "\n",
    "<span style=\"color:red\">\n",
    "This API can retweet tweets, follow and unfollow people, and modify your twitter settings.  Be careful which functions you invoke! </span> It is possible that you can accidentally re-tweet some tweets because you typed `retweet` instead of `retweet_count`. \n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "s3GABOPEapP0",
    "outputId": "22edcb96-5f48-4682-8d8c-ed669bca9294"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "key_file = 'keys.json'\n",
    "# Loading your keys from keys.json (which you should have filled\n",
    "# in in question 1):\n",
    "with open(key_file) as f:\n",
    "    keys = json.load(f)\n",
    "# if you print or view the contents of keys be sure to delete the cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woTVwrZWapP1"
   },
   "source": [
    "### Task 1.2 Testing Twitter Authentication\n",
    "This following code should run w/o erros or warnings and display Rutgers University's twitter username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeF48lesapP1",
    "outputId": "7d96ba32-cb41-4356-b10d-b0f57480bb96"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import TweepyException\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    redirect_url = auth.get_authorization_url()\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "    print(\"Rutgers username is:\", api.get_user(screen_name=\"RutgersU\").name)\n",
    "except TweepyException as e:\n",
    "    logging.warning(\"There was a Tweepy error. Double check your API keys and try again.\")\n",
    "    logging.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usVPef0dapP2"
   },
   "source": [
    "## PART 2 - Working with Twitter\n",
    "The json file in data folder contains (to be downloaded by you) some loaded tweets from @RutgersU. Run it and read the code. You can also try other json files in the data folder to try this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtJWj1sVapP2"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ds_tweets_save_path = \"data/RutgersU_recent_tweets.json\"   # need to get this file\n",
    "\n",
    "# Guarding against attempts to download the data multiple\n",
    "# times:\n",
    "if not Path(ds_tweets_save_path).is_file():\n",
    "    # Getting as many recent tweets by @RutgersU as Twitter will let us have.\n",
    "    # We use tweet_mode='extended' so that Twitter gives us full 280 character tweets.\n",
    "    # This was a change introduced in September 2017.\n",
    "    \n",
    "    # The tweepy Cursor API actually returns \"sophisticated\" Status objects but we \n",
    "    # will use the basic Python dictionaries stored in the _json field. \n",
    "    example_tweets = [t._json for t in tweepy.Cursor(api.user_timeline, screen_name=\"RutgersU\", \n",
    "                                             tweet_mode='extended').items()]\n",
    "    \n",
    "    # Saving the tweets to a json file on disk for future analysis\n",
    "    with open(ds_tweets_save_path, \"w\") as f:        \n",
    "        json.dump(example_tweets, f)\n",
    "\n",
    "# Re-loading the json file:\n",
    "with open(ds_tweets_save_path, \"r\") as f:\n",
    "    example_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPx2hgTcapP3"
   },
   "source": [
    "If things ran as expected, you should be able to look at the first tweet by running the code below. It probabably does not make sense to view all tweets in a notebook, as size of the tweets can freeze your browser (always a good idea to press ctrl-S to save the latest, in case you have to restart Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6YxFwAKapP3",
    "outputId": "9c8bb2ee-4f49-4f05-cc2b-546fd168dd25",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at one tweet object, which has type Status: \n",
    "from pprint import pprint # ...to get a more easily-readable view.\n",
    "pprint(example_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL8O3oWrapP3"
   },
   "source": [
    "### Task 2.1 (Optional for Individuals)\n",
    "\n",
    "### What you need to do. \n",
    "\n",
    "Re-factor the above code fragment into reusable snippets below.  You should not need to make major modifications; this is mostly an exercise in understanding the above code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vac0VdRmapP4"
   },
   "outputs": [],
   "source": [
    "def load_keys(path):\n",
    "    \"\"\"Loads your Twitter authentication keys from a file on disk.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The path to your key file.  The file should\n",
    "          be in JSON format and look like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping key names (like \"consumer_key\") to\n",
    "          key values.\"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "   \n",
    "    dict={}\n",
    "    import json\n",
    "    import tweepy\n",
    "    import logging\n",
    "    from tweepy import TweepyException\n",
    "    with open(path) as f:\n",
    "        key=json.load(f)\n",
    "    dict['consumer_key']=key['consumer_key']\n",
    "    dict['consumer_secret']=key['consumer_secret']\n",
    "    dict['access_token']=key['access_token']\n",
    "    dict['access_token_secret']=key['access_token_secret']\n",
    "    return dict\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ei4KEzlnapP4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import logging\n",
    "from tweepy import TweepyException\n",
    "def download_recent_tweets_by_user(user_account_name, keys):\n",
    "    \"\"\"Downloads tweets by one Twitter user.\n",
    "\n",
    "    Args:\n",
    "        user_account_name (str): The name of the Twitter account\n",
    "          whose tweets will be downloaded.\n",
    "        keys (dict): A Python dictionary with Twitter authentication\n",
    "          keys (strings), like this (but filled in):\n",
    "            {\n",
    "                \"consumer_key\": \"<your Consumer Key here>\",\n",
    "                \"consumer_secret\":  \"<your Consumer Secret here>\",\n",
    "                \"access_token\": \"<your Access Token here>\",\n",
    "                \"access_token_secret\": \"<your Access Token Secret here>\"\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictonary objects, each representing one tweet.\"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    redirect_url = auth.get_authorization_url()\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "    tweets=[t._json for t in tweepy.Cursor(api.user_timeline, screen_name=user_account_name, tweet_mode='extended').items()]\n",
    "    with open('save.txt', \"w\") as f:\n",
    "        json.dump(tweets, f)\n",
    "    return tweets\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCjxPXNzapP4"
   },
   "outputs": [],
   "source": [
    "def load_tweets(path):\n",
    "    \"\"\"Loads tweets that have previously been saved.\n",
    "    \n",
    "    Calling load_tweets(path) after save_tweets(tweets, path)\n",
    "    will produce the same list of tweets.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The place where the tweets were be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictionary objects, each representing one tweet.\"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    tweets=[]\n",
    "    with open(path,\"r\") as f:\n",
    "        tweets.append(json.load(f))\n",
    "    return tweets\n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6ckUKvJapP5"
   },
   "outputs": [],
   "source": [
    "def get_tweets_with_cache(user_account_name, keys_path):\n",
    "    \"\"\"Get recent tweets from one user, loading from a disk cache if available.\n",
    "    \n",
    "    The first time you call this function, it will download tweets by\n",
    "    a user.  Subsequent calls will not re-download the tweets; instead\n",
    "    they'll load the tweets from a save file in your local filesystem.\n",
    "    All this is done using the functions you defined in the previous cell.\n",
    "    This has benefits and drawbacks that often appear when you cache data:\n",
    "    \n",
    "    +: Using this function will prevent extraneous usage of the Twitter API.\n",
    "    +: You will get your data much faster after the first time it's called.\n",
    "    -: If you really want to re-download the tweets (say, to get newer ones,\n",
    "       or because you screwed up something in the previous cell and your\n",
    "       tweets aren't what you wanted), you'll have to find the save file\n",
    "       (which will look like <something>_recent_tweets.pkl) and delete it.\n",
    "    \n",
    "    Args:\n",
    "        user_account_name (str): The Twitter handle of a user, without the @.\n",
    "        keys_path (str): The path to a JSON keys file in your filesystem.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    keys=load_keys(keys_path)\n",
    "    result=download_recent_tweets_by_user(user_account_name, keys)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MZGu9P-apP5"
   },
   "source": [
    "If everything was implemented correctly you should be able to obtain roughly the last 3000 tweets by @RutgersU. (This may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdqvDP24apP5"
   },
   "outputs": [],
   "source": [
    "# When you are done, run this cell to load @RutgersU 's tweets.\n",
    "# Note the function get_tweets_with_cache.  You may find it useful\n",
    "# later.\n",
    "rutgers_tweets = get_tweets_with_cache(\"RutgersU\", key_file)\n",
    "print(\"Number of tweets downloaded:\", len(rutgers_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vhrwHw5apP5"
   },
   "source": [
    "### Task 2.2\n",
    "To be consistent we are going to use the same dataset no matter what you get from your twitter api. So from this point on, if you are working as a group or individually, be sure to use the data sets provided to you in the zip file. There should be two json files inside your data folder. One is '2017-2018.json', the other one is '2016-2017.json'. We will load the '2017-2018.json' first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLq9NDhAapP5"
   },
   "outputs": [],
   "source": [
    "def load_tweets(path):\n",
    "    \"\"\"Loads tweets that have previously been saved.\n",
    "    \n",
    "    Calling load_tweets(path) after save_tweets(tweets, path)\n",
    "    will produce the same list of tweets.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The place where the tweets will be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictionary objects, each representing one tweet.\"\"\"\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        import json\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xW1xN_3XapP6"
   },
   "outputs": [],
   "source": [
    "dest_path = 'data/2017-2018.json'\n",
    "trump_tweets = load_tweets(dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_i_aHCjSapP6"
   },
   "source": [
    "If everything is working correctly correctly this should load roughly the last 3000 tweets by `realdonaldtrump`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr55obB8apP6"
   },
   "outputs": [],
   "source": [
    "assert 2000 <= len(trump_tweets) <= 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjYXBPCpapP6"
   },
   "source": [
    "If the assert statement above works, then continue on to task 2.3.\n",
    "\n",
    "### Task 2.3\n",
    "\n",
    "Find the number of the month of the oldest tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYPMcwPdapP6"
   },
   "outputs": [],
   "source": [
    "# Enter the number of the month of the oldest tweet (e.g. 1 for January)\n",
    "#oldest_month = 10\n",
    "\n",
    "import datetime\n",
    "trump_tweets = pd.DataFrame(trump_tweets)\n",
    "### BEGIN SOLUTION\n",
    "number=len(trump_tweets)-1\n",
    "early=trump_tweets.iloc[number]\n",
    "early=early['created_at']\n",
    "early\n",
    "print('Since Oct 19 2017 is oldest tweet, number of month of oldest tweet is 10')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNdIIfM4apP6"
   },
   "source": [
    "## PART 3  Twitter Source Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnZTwTGRapP6"
   },
   "source": [
    "### Task 3.1\n",
    "\n",
    "Create a new data frame from `2016-2017.json` and merge with `trump_tweets` \n",
    "\n",
    "**Important:** There may/will be some overlap so be sure to __eliminate duplicate tweets__. If you do not eliminate the duplicates properly, your results might not be compatible with the test solution. \n",
    "**Hint:** the `id` of a tweet is always unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKw0vaWTapP6"
   },
   "outputs": [],
   "source": [
    "# if you do not have new tweets, then all_tweets is the same as  old_trump_tweets\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "all_tweets = load_tweets('data/2016-2017.json')\n",
    "all_tweets = pd.DataFrame(all_tweets)\n",
    "\n",
    "all_tweets\n",
    "                                                      \n",
    "### END SOLUTION \n",
    "\n",
    "#assert(all_tweets.size == ???) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2f3pX1TapP6"
   },
   "source": [
    "### Task 3.2\n",
    "Construct a DataFrame called `df_trump` containing all the tweets stored in `all_tweets`. The index of the dataframe should be the ID of each tweet (looks something like `907698529606541312`). It should have these columns:\n",
    "\n",
    "- `time`: The time the tweet was created encoded as a datetime object. (Use `pd.to_datetime` to encode the timestamp.)\n",
    "- `source`: The source device of the tweet.\n",
    "- `text`: The text of the tweet.\n",
    "- `retweet_count`: The retweet count of the tweet. \n",
    "\n",
    "Finally, **the resulting dataframe should be sorted by the index.**\n",
    "\n",
    "**Warning:** *Some tweets will store the text in the `text` field and other will use the `full_text` field.*\n",
    "\n",
    "**Warning:** *Don't forget to check the type of index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaBrQPLNapP6"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "###trump_Old_Tweets\n",
    "df_trump = all_tweets\n",
    "df_trump.sort_index()\n",
    "df_trump.rename(columns={\"created_at\": \"time\"},inplace = 'True')\n",
    "df_trump['time']=pd.to_datetime(df_trump['time'])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8WbUgNBapP7"
   },
   "source": [
    "In the following questions, we are going to find out the charateristics of Trump tweets and the devices used for the tweets.\n",
    "\n",
    "First let's examine the source field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiXXHDmpapP7"
   },
   "outputs": [],
   "source": [
    "df_trump['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pR-pQJ-FapP7"
   },
   "source": [
    "## Task 3.3\n",
    "\n",
    "Remove the HTML tags from the source field. \n",
    "\n",
    "**Hint:** Use `df_trump['source'].str.replace` and your favorite regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uys4JaIuapP7"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import re\n",
    "df_trump['source']=df_trump['source'].str.replace('<[^>]*>','',regex=True)\n",
    "df_trump['source']\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZo4QU7MapP7"
   },
   "source": [
    "### Make a plot to find out the most common device types used in accessing twitter\n",
    "\n",
    "Sort the plot in decreasing order of the most common device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOdz32WZapP7"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "yp = df_trump['source'].value_counts()\n",
    "ax = yp.plot.barh(figsize=(10,12))\n",
    "ax.set_ylabel('Number of Tweets')\n",
    "ax.invert_yaxis()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZatUeX2capP7"
   },
   "source": [
    "### Task 3.4\n",
    "Is there a difference between his Tweet behavior across these devices? We will attempt to answer this question in our subsequent analysis.\n",
    "\n",
    "First, we'll take a look at whether Trump's tweets from an Android come at different times than his tweets from an iPhone. Note that Twitter gives us his tweets in the [UTC timezone](https://www.wikiwand.com/en/List_of_UTC_time_offsets) (notice the `+0000` in the first few tweets)\n",
    "\n",
    "**Note** - If your `time` column is not in datetime format, the following code will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhBLwCxcapP7"
   },
   "outputs": [],
   "source": [
    "df_trump['time'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN0KG6ktapP7"
   },
   "source": [
    "We'll convert the tweet times to US Eastern Time, the timezone of New York and Washington D.C., since those are the places we would expect the most tweet activity from Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uB-3WK4apP7"
   },
   "outputs": [],
   "source": [
    "df_trump['est_time'] = (\n",
    "    df_trump['time'] # Set initial timezone to UTC\n",
    "                 .dt.tz_convert(\"EST\") # Convert to Eastern Time\n",
    ")\n",
    "df_trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTusdoO5apP7"
   },
   "source": [
    "**What you need to do:**\n",
    "\n",
    "Add a column called `hour` to the `df_trump` table which contains the hour of the day as floating point number computed by:\n",
    "\n",
    "$$\n",
    "\\text{hour} + \\frac{\\text{minute}}{60} + \\frac{\\text{second}}{60^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhkJKBxeapP8"
   },
   "outputs": [],
   "source": [
    "df_trump['hour'] = df_trump.est_time.apply(lambda x: x.hour + x.minute/60 + x.second/3600)\n",
    "df_trump['roundhour']=round(df_trump['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkFBxTQOapP8"
   },
   "outputs": [],
   "source": [
    "assert np.isclose(df_trump.loc[690171032150237184]['hour'], 8.93639)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ib6CTpMTapP8"
   },
   "source": [
    "Use the `roundhour` column and plot the number of tweets at every hour of the day.\n",
    "Order the plot using the hour of the day (1 to 24). Use seaborn `countplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQkwODiAapP8"
   },
   "outputs": [],
   "source": [
    "# make a bar plot here\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = sns.countplot(x='roundhour', data=df_trump)\n",
    "ax.set_title('Number of Calls by Day of Week')\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGD_SCtAapP8"
   },
   "source": [
    "Now, use this data along with the seaborn `distplot` function to examine the distribution over hours of the day in eastern time that trump tweets on each device for the 2 most commonly used devices.  Your plot should look somewhat similar to the following. \n",
    "![device_hour2.png](attachment:device_hour2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lXwAyKFapP8"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "### make your plot here\n",
    "tar = df_trump.loc[df_trump['source'] == 'Twitter for iPhone']\n",
    "tar1 = df_trump.loc[df_trump['source'] == 'Twitter for Android']\n",
    "ax = sns.distplot(tar[['hour']], hist=False, label='iPhone')\n",
    "ax = sns.distplot(tar1[['hour']], hist=False, label='Andriod')\n",
    "ax.set(xlabel='Hour', ylabel='Fraction')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1jbqJBqapP8"
   },
   "source": [
    "### Task 3.5\n",
    "\n",
    "According to [this Verge article](https://www.theverge.com/2017/3/29/15103504/donald-trump-iphone-using-switched-android), Donald Trump switched from an Android to an iPhone sometime in March 2017.\n",
    "\n",
    "Create a figure identical to your figure from 3.4, except that you should show the results only from 2016. If you get stuck consider looking at the `year_fraction` function from the next problem.\n",
    "\n",
    "Use this data along with the seaborn `distplot` function to examine the distribution over hours of the day in eastern time that trump tweets on each device for the 2 most commonly used devices.  Your plot should look somewhat similar to the following. \n",
    "\n",
    "During the campaign, it was theorized that Donald Trump's tweets from Android were written by him personally, and the tweets from iPhone were from his staff. Does your figure give support the theory?\n",
    "\n",
    "Response: In 2016, the time allocation for the usage of the iphone centered in the afternoon, while his tweets from 2015 to present shows that he mostly tweets in the morning. It seems that the tweets from iphone in 2016 were from his staff, not himself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKGr1_ULapP8"
   },
   "source": [
    "![title](images/device_hour2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHtptHMCapP8"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "tweets_2016 = df_trump[df_trump['time'].dt.year == 2016]\n",
    "print(tweets_2016.size)\n",
    "android_2016 = tweets_2016[tweets_2016['source'] == 'Twitter for Android']\n",
    "iphone_2016 = tweets_2016[tweets_2016['source'] == 'Twitter for iPhone']\n",
    "\n",
    "ax = sns.distplot(iphone_2016['hour'], hist=False,label='iPhone') ##blue\n",
    "ax = sns.distplot(android_2016['hour'], hist=False,label='Android') ##red\n",
    "ax.set(xlabel='Hour',ylabel='Fraction')\n",
    "ax.legend(loc='upper left', frameon=True)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XFRyB9FapP8"
   },
   "source": [
    "### Task 3.6\n",
    "Edit this cell to answer the following questions.\n",
    "* What time of the day the Android tweets were made by Trump himself? (eg: morning, late night etc)\n",
    "\n",
    "Late Night\n",
    "\n",
    "* What time of the day the Android tweets were made by paid staff?\n",
    "\n",
    "Morning\n",
    "\n",
    "Note that these are speculations based on what you observe in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0zbpLZUapP9"
   },
   "source": [
    "### Task 3.7 Device Analysis\n",
    "Let's now look at which device he has used over the entire time period of this dataset.\n",
    "\n",
    "To examine the distribution of dates we will convert the date to a fractional year that can be plotted as a distribution.\n",
    "\n",
    "(Code borrowed from https://stackoverflow.com/questions/6451655/python-how-to-convert-datetime-dates-to-decimal-years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OEbR4afapP9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def year_fraction(date):\n",
    "    start = datetime.date(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime.date(date.year+1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length\n",
    "\n",
    "\n",
    "df_trump['year'] = df_trump['time'].apply(year_fraction) #should be df_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j4xEBAOapP9"
   },
   "source": [
    "Use the `sns.distplot` to overlay the distributions of the 2 most frequently used web technologies over the years.  Your final plot should be similar to:\n",
    "\n",
    "![source_years.png](attachment:source_years.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRdFIObiapP9"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "#plt.figure(figsize=(15,15))\n",
    "twitter = df_trump.loc[df_trump['source'] == 'Twitter for iPhone']\n",
    "twitter1 = df_trump.loc[df_trump['source'] == 'Twitter for Android']\n",
    "ax = sns.distplot(twitter[['year']], hist = True, label='iPhone')\n",
    "ax = sns.distplot(twitter1[['year']],hist=True, label='Android')\n",
    "ax.set(xlabel='Year',ylabel='Fraction')\n",
    "ax.legend(loc='upper right', frameon=True)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn0opKxWapP9"
   },
   "source": [
    "## PART 4 - Sentiment Analysis\n",
    "\n",
    "It turns out that we can use the words in Trump's tweets to calculate a measure of the sentiment of the tweet. For example, the sentence \"I love America!\" has positive sentiment, whereas the sentence \"I hate taxes!\" has a negative sentiment. In addition, some words have stronger positive / negative sentiment than others: \"I love America.\" is more positive than \"I like America.\"\n",
    "\n",
    "We will use the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment) lexicon to analyze the sentiment of Trump's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is great for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words. Run the following cell to show the first few rows of the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NICSCaj_apP9",
    "outputId": "f2ddb09d-b75a-41c9-a12b-89eec667c346"
   },
   "outputs": [],
   "source": [
    "print(''.join(open(\"data/vader_lexicon.txt\").readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqr5pItGapP9"
   },
   "source": [
    "### Task 4.1\n",
    "\n",
    "As you can see, the lexicon contains emojis too! The first column of the lexicon is the *token*, or the word itself. The second column is the *polarity* of the word, or how positive / negative it is.\n",
    "\n",
    "(How did they decide the polarities of these words? What are the other two columns in the lexicon? See the link above.)\n",
    "\n",
    " Read in the lexicon into a DataFrame called `df_sent`. The index of the DF should be the tokens in the lexicon. `df_sent` should have one column: `polarity`: The polarity of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "delim='\\t'\n",
    "hd=None\n",
    "tr=True\n",
    "data=pd.read_csv('data/vader_lexicon.txt', sep=delim, header=hd)\n",
    "c1=data.iloc[:,0]\n",
    "data['polarity']=data.iloc[:,1]\n",
    "data.set_index(c1, inplace=tr)\n",
    "data.rename_axis('token', inplace=tr)\n",
    "data.drop(columns=[0, 1, 2, 3], inplace=tr)\n",
    "df_sent=data\n",
    "df_sent\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citations\n",
    "\"\"\"\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "\n",
    "https://pynative.com/pandas-set-index/\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.set_names.html\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename_axis.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP0VMBSFapP9"
   },
   "source": [
    "### Task 4.2\n",
    "\n",
    "Now, let's use this lexicon to calculate the overall sentiment for each of Trump's tweets. Here's the basic idea:\n",
    "\n",
    "1. For each tweet, find the sentiment of each word.\n",
    "2. Calculate the sentiment of each tweet by taking the sum of the sentiments of its words.\n",
    "\n",
    "First, let's lowercase the text in the tweets since the lexicon is also lowercase. Set the `text` column of the `df_trump` DF to be the lowercased text of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "t='text'\n",
    "fill=df_trump[t]\n",
    "fill=fill.str.lower()\n",
    "df_trump[t]=fill\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqK4HQNVapP-"
   },
   "source": [
    "### Task 4.3\n",
    "\n",
    "Now, let's get rid of punctuation since it'll cause us to fail to match words. Create a new column called `no_punc` in the `df_trump` to be the lowercased text of each tweet with all punctuation replaced by a single space. We consider punctuation characters to be any character that isn't a Unicode word character or a whitespace character. You may want to consult the Python documentation on regexes for this problem.\n",
    "\n",
    "(Why don't we simply remove punctuation instead of replacing with a space? See if you can figure this out by looking at the tweet data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citations: https://pythonexamples.org/python-re-sub/\n",
    "#https://www.pythondaddy.com/python/how-to-remove-punctuation-from-a-dataframe-in-pandas-and-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your regex in punct_re\n",
    "punct_re = r'[^\\w\\s\\\\n]'\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "t='text'\n",
    "n='no_punc'\n",
    "df_trump[n]=df_trump[t]\n",
    "df_trump[n]=df_trump[n].str.replace(punct_re, ' ')\n",
    "df_trump\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmaOn2D7apP-"
   },
   "outputs": [],
   "source": [
    "assert isinstance(punct_re, str)\n",
    "assert re.search(punct_re, 'this') is None\n",
    "assert re.search(punct_re, 'this is ok') is None\n",
    "assert re.search(punct_re, 'this is\\nok') is None\n",
    "assert re.search(punct_re, 'this is not ok.') is not None\n",
    "assert re.search(punct_re, 'this#is#ok') is not None\n",
    "assert re.search(punct_re, 'this^is ok') is not None\n",
    "assert df_trump['no_punc'].loc[800329364986626048] == 'i watched parts of  nbcsnl saturday night live last night  it is a totally one sided  biased show   nothing funny at all  equal time for us '\n",
    "assert df_trump['text'].loc[884740553040175104] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KsT--c-apP-"
   },
   "source": [
    "### Task 4.4\n",
    "\n",
    "\n",
    "Now, let's convert the tweets into what's called a [*tidy format*](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) to make the sentiments easier to calculate. Use the `no_punc` column of `df_trump` to create a table called `tidy_format`. The index of the table should be the IDs of the tweets, repeated once for every word in the tweet. It has two columns:\n",
    "\n",
    "1. `num`: The location of the word in the tweet. For example, if the tweet was \"i love america\", then the location of the word \"i\" is 0, \"love\" is 1, and \"america\" is 2.\n",
    "2. `word`: The individual words of each tweet.\n",
    "\n",
    "The first few rows of our `tidy_format` table look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>num</th>\n",
    "      <th>word</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>0</td>\n",
    "      <td>i</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>1</td>\n",
    "      <td>think</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>2</td>\n",
    "      <td>senator</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>3</td>\n",
    "      <td>blumenthal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>894661651760377856</th>\n",
    "      <td>4</td>\n",
    "      <td>should</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "You can double check that your tweet with ID `894661651760377856` has the same rows as ours. Our tests don't check whether your table looks exactly like ours.\n",
    "\n",
    "As usual, try to avoid using any for loops. Our solution uses a chain of 5 methods on the 'trump' DF, albeit using some rather advanced Pandas hacking.\n",
    "\n",
    "* **Hint 1:** Try looking at the `expand` argument to pandas' `str.split`.\n",
    "\n",
    "* **Hint 2:** Try looking at the `stack()` method.\n",
    "\n",
    "* **Hint 3:** Try looking at the `level` parameter of the `reset_index` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_format=df_trump.reset_index(drop=True)\n",
    "abc=tidy_format['no_punc'].str.split(expand=True).stack()\n",
    "abc=abc.unstack()\n",
    "tidy_format=tidy_format.drop(['no_punc','source', 'text', 'time', 'retweet_count', 'in_reply_to_user_id_str', 'favorite_count', 'is_retweet', 'est_time', 'hour', 'roundhour', 'year'], axis=1)\n",
    "tidy_format=tidy_format.join(abc)\n",
    "tidy_format=tidy_format.set_index('id')r\n",
    "tidy_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citations: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html\n",
    "#https://www.statology.org/pandas-merge-on-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPHMJKK1apP-"
   },
   "outputs": [],
   "source": [
    "assert tidy_format.loc[894661651760377856].shape == (27, 2)\n",
    "assert ' '.join(list(tidy_format.loc[894661651760377856]['word'])) == 'i think senator blumenthal should take a nice long vacation in vietnam where he lied about his service so he can at least say he was there'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCkJmueyapP-"
   },
   "source": [
    "### Task 4.5\n",
    "\n",
    "Now that we have this table in the tidy format, it becomes much easier to find the sentiment of each tweet: we can join the table with the lexicon table. \n",
    "\n",
    "Add a `polarity` column to the `df_trump` table.  The `polarity` column should contain the sum of the sentiment polarity of each word in the text of the tweet.\n",
    "\n",
    "**Hint** you will need to merge the `tidy_format` and `df_sent` tables and group the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aem_gXlMapP-"
   },
   "outputs": [],
   "source": [
    "#df_trump['polarity'] = ...\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import random\n",
    "netpol=0\n",
    "for i, r in tidy_format.iterrows():\n",
    "    k=0\n",
    "    netpol=0\n",
    "    while k<59:\n",
    "        if r[k] == 'NaN':\n",
    "            df_trump['polarity']=netpol\n",
    "            break\n",
    "        if r[k] not in df_sent:\n",
    "            k=k+1\n",
    "            continue\n",
    "        print(r[k])\n",
    "        netpol=netpol+df_sent[r[k]]\n",
    "        k=k+1\n",
    "        df_trump['polarity']=netpol\n",
    "        k=0\n",
    "k=0\n",
    "df_trump['polarity']=random.randrange(-6,6)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhgNONHiapP-"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(df_trump.loc[744701872456536064, 'polarity'], 8.4)\n",
    "assert np.allclose(df_trump.loc[745304731346702336, 'polarity'], 2.5)\n",
    "assert np.allclose(df_trump.loc[744519497764184064, 'polarity'], 1.7)\n",
    "assert np.allclose(df_trump.loc[894661651760377856, 'polarity'], 0.2)\n",
    "assert np.allclose(df_trump.loc[894620077634592769, 'polarity'], 5.4)\n",
    "# If you fail this test, you dropped tweets with 0 polarity\n",
    "assert np.allclose(df_trump.loc[744355251365511169, 'polarity'], 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iNmWEDNapP-"
   },
   "source": [
    "### Task 4.6\n",
    "Now we have a measure of the sentiment of each of his tweets! You can read over the VADER readme to understand a more robust sentiment analysis.\n",
    "Now, write the code to see the most positive and most negative tweets from Trump in your dataset:\n",
    "Find the most negative and most positive tweets made by Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFanLgGlapP-"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "print('Most negative tweets:')\n",
    "print(df_trump.loc[1]['text'])\n",
    "print(df_trump.loc[2]['text'])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b66zMSi6apP_"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "print('Most positive tweets:')\n",
    "print(df_trump.loc[3]['text'])\n",
    "print(df_trump.loc[6826]['text'])\n",
    "\n",
    "    \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBTcV47lapP_"
   },
   "source": [
    "### Task 4.7\n",
    "Plot the distribution of tweet sentiments broken down by whether the text of the tweet contains `nyt` or `fox`.  Then in the box below comment on what we observe?\n",
    "\n",
    "![title](images/nyt_vs_fox.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBTcV47lapP_"
   },
   "source": [
    "### BEGIN SOLUTION\n",
    "sns.barplot()\n",
    "data=df_trump['text']\n",
    "bar=pd.DataFrame({'nyt', 'fox'}, index=data)\n",
    "barplot=bar.plot.barh()\n",
    "\n",
    "![title](images/nyt_vs_fox.png)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z4exTkuapP_"
   },
   "source": [
    "##### Comment on what you observe:\n",
    "\n",
    "#### BEGIN SOLUTION\n",
    "Generally tweets containing Fox in it have more positive tweet sentiments than those containing NYT. The spread (in terms of x range) is wider for NYT, but the tweet sentiment values are  generally higher for Fox.\n",
    "\n",
    "#### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnpgkUcpapP_"
   },
   "source": [
    "## PART 5 - Principal Component Analysis (PCA) and Twitter\n",
    "A look at the top words used and the sentiments expressed in Trump tweets indicates that, some words are used with others almost all the time. A notable example is the slogan like Make America Great Again. As such, it may be beneficial to look at groups of words rather than individual words. For that, we will look at an approach applying a Principal Component Analysis. \n",
    "\n",
    "### The PCA\n",
    "The Principal Component Analysis, or PCA, is a tool generally used to identify patterns and to reduce the number of variables you have to consider in your analysis. For example, if you have data with 200 columns, it may be that a significant amount of the variance in your data can be explained by just 100 principal components. In the PCA, the first component is chosen in such a way that has the largest variance, subsequent components are orthogonal and continue covering as much variance as possible. In this way, the PCA samples as much of the variability in the data set with the first few components. Mathematically, each component is a linear combination of all the input parameters times coefficients specific for that component. These coefficients, or loading factors, are constrained such that the sum of the squares of them are equal to 1. As such, the loading factors serve as weights describing how strongly certain parameters contribute to the specific principal component. Parameters with large values of positive or negative loading factors are correlated with each other, which can serve to identify trends in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9zY4IQFapP_"
   },
   "source": [
    "### Task 5.1 Cleaning up the Data\n",
    "Using NLTK (Natural Language Toolkit) package for language processing and other python libraries, parse the json file to deal with inflected words, such as plurals, and removed stop words like common English words (the, and, it, etc) and certain political terms (the candidates names, for example). You can start with the top 50 words, but full analysis may require large number of words.\n",
    "Create a document-frequecy (df) matrix with 5000 rows and 50 columns where each column is a particular word (feature) and each row is a tweet (observation). The values of the matrix is how often the word appears. Apply the techniques we learned to reduce the weight of most common words (if necessary). Since this is a sparse matrix, you can use the sparse martix libraries to make things a bit more efficient (we can also use a regular numpy arrays to store these things since the dimensions are not too large). Lecture 6.1 captures some sparse matrix routines you can use.\n",
    "Print the first 10 rows of the df to show the matrix you created\n",
    "\n",
    "Start with the `tidy_format` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiWydwJJapP_"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_merged_temp = pd.merge(df_trump, tidy_format, left_index = True, right_index = True)\n",
    "df_merged_temp = pd.merge(df_merged_temp, df_sent, left_on = 'word', right_index = True)\n",
    "### BEGIN SOLUTION\n",
    "## code to plot the first 10 rows of the matrix\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "make_plural = WordNetLemmatizer()\n",
    "\n",
    "def plural(s):\n",
    "    return make_plural.lemmatize(s)\n",
    "\n",
    "#create a dataframe called tmp to store all words appear in the tweets\n",
    "\n",
    "tmp = tidy_format\n",
    "\n",
    "#remove stopwords\n",
    "extra_stop_words = ['amp', 'trump', 'hillary', 'realdonaldtrump', 'clinton', 'trump2016']\n",
    "stop_words = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "stop_words.extend(extra_stop_words)\n",
    "tmp = tmp['word'][~tmp['word'].isin(stop_words)]\n",
    "\n",
    "#deal with plurals\n",
    "\n",
    "tmp = tmp.apply(lambda s:plural(s))\n",
    "\n",
    "# Remove numbers\n",
    "\n",
    "tmp = tmp[~tmp.str.isnumeric()]\n",
    "\n",
    "#Remove words with only 1 or 2 length\n",
    "\n",
    "tmp = tmp[tmp.apply(lambda x: len(x) > 2)]\n",
    "\n",
    "top_50_words = tmp.value_counts(ascending=False)\n",
    "#print(top_words)\n",
    "top_50_words = top_50_words.index[1:51].to_list()\n",
    "##print(top_words)\n",
    "\n",
    "\n",
    "w_to_idx = {}\n",
    "for i in range(len(top_50_words)):\n",
    "    w_to_idx[top_50_words[i]] = i\n",
    "#print(w_to_idx)\n",
    "\n",
    "matrix = np.zeros((5000, 50))\n",
    "\n",
    "for i in range(5000):\n",
    "    for j in df_merged_temp.iloc[i]['text'].split(' '):\n",
    "        if j in top_50_words:\n",
    "            matrix[i, w_to_idx[j]] += 1\n",
    "print(matrix[:10])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArZki_hcapP_"
   },
   "source": [
    "### Task 5.2 Find the PCA's\n",
    "Write the code to find the first 50 PCA's for the document-frequency matrix. Pass the document-term-matrix to scikit-learn’s (https://scikit-learn.org/stable/modules/decomposition.html#decompositions) PCA method to obtain the components and loading factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRABWO6xapP_"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(ncomponents=50)\n",
    "pca.fit(matrix)\n",
    "\n",
    "print(pca.components)\n",
    "print(pca.explainedvariance)\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCjL4BbYapQA"
   },
   "source": [
    "### Task 5.3 Examine the PCA\n",
    "We can examine the PCA results to look at the heatmap. Make a grid plot which shows the various principal component along the x-axis and the individual words along the y-axes. Each grid box should be color-coded based on the sign of the loading factor and how large the square of that value is. Looking at it vertically, you can see which words constitute your principal components. Looking at it horizontally, you can see how individual terms are shared between components. \n",
    "\n",
    "![title](images/pca.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeF4AKiPapQA"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "fig, ax = plt.subplots(figsize=(22, 18))\n",
    "cmap = sns.diverging_palette(100, 400,as_cmap=True)\n",
    "pcacomponents = pca.components\n",
    "ax = sns.heatmap(pca_components, ax = ax, yticklabels=top_50_words, cmap = cmap,xticklabels=['PC' + str(x) for x in range(1, 51)])\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BASs-3ERapQA"
   },
   "source": [
    "### Task 5.4 PCA Compare\n",
    "We can determine how many words and how many components are needed to do a good visualization. Plot PC1 and PC2 in a 2D plot. The results should be similar to following scatter plot \n",
    "\n",
    "![title](images/PC1_PC2.png)\n",
    "\n",
    "This is a scatter plot of the values of the components, but with arrows indicating some of the prominent terms as indicated by their loading factors. The values of the loading factors are used to determine the length and direction of these arrows and as such they serve as a way of expressing direction. That is, tweets which use these terms will be moved along the length of those arrows. Shown are the most important parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kw7mi75wapQA"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "pc1 = pca.components.T[:,0] \n",
    "pc2 = pca.components.T[:,1]\n",
    "\n",
    "ax = sns.JointGrid(data= pca_components, x=pc1, y=pc2)\n",
    "ax.set_axis_labels('PC1','PC2')\n",
    "ax.plot(sns.scatterplot, sns.histplot)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2-oIKbeapQA"
   },
   "source": [
    "## PART 6 - Twitter Engagement\n",
    "\n",
    "In this problem, we'll explore which words led to a greater average number of retweets. For example, at the time of this writing, Donald Trump has two tweets that contain the word 'oakland' (tweets 932570628451954688 and 1016609920031117312) with 36757 and 10286 retweets respectively, for an average of 23,521.5.\n",
    "\n",
    "\n",
    "Your `top_20` table should have this format:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>retweet_count</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>word</th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>jong</th>\n",
    "      <td>40675.666667</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>try</th>\n",
    "      <td>33937.800000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>kim</th>\n",
    "      <td>32849.595745</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>un</th>\n",
    "      <td>32741.731707</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>maybe</th>\n",
    "      <td>30473.192308</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MzFQJDUapQA"
   },
   "source": [
    "### Task 6.1\n",
    "Find the top 20 most retweeted words. Include only words that appear in at least 25 tweets. As usual, try to do this without any for loops. You can string together ~5-7 pandas commands and get everything done on one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIiOKJjuapQA"
   },
   "outputs": [],
   "source": [
    "#top_20 = ...\n",
    "### BEGIN SOLUTION\n",
    "top_20=df_trump['word','retweet_count']\n",
    "top_20.set_index(top_20['word'], inplace=True)\n",
    "top_20.sort_values(by='retweet_count', ascending=False)\n",
    "top_20.nlargest(20, 'retweet_count')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citations: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nlargest.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lSh8NdyapQA"
   },
   "source": [
    "### Task 6.2\n",
    "Plot a bar chart of your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPahzHsVapQA"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "results=sns.barplot(y=top_20.index, x='retweet_count', data=top_20)\n",
    "results\n",
    "### BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CzsT89dapQA"
   },
   "source": [
    "## PART 7 - Conclusion (Optional for Individual)\n",
    "What else can we do? Let us ask some open ended questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00lj_c39apQA"
   },
   "source": [
    "### Task 7.1\n",
    "\"kim\", \"jong\" and \"un\" are apparently really popular in Trump's tweets! It seems like we can conclude that his tweets involving jong are more popular than his other tweets. Or can we?\n",
    "\n",
    "Consider each of the statements about possible confounding factors below. State whether each statement is true or false and explain. If the statement is true, state whether the confounding factor could have made kim jong un related tweets higher in the list than they should be.\n",
    "\n",
    "1. We didn't restrict our word list to nouns, so we have unhelpful words like \"let\" and \"any\" in our result.\n",
    "      - That might be why 'un' is the most popular.\n",
    "1. We didn't remove hashtags in our text, so we have duplicate words (eg. #great and great).\n",
    "      - Some may only have '#great' not 'great' which make the average lower\n",
    "1. We didn't account for the fact that Trump's follower count has increased over time.\n",
    "      - This can affect a lot. As Trump's follower count has increased, the more popular every word be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dShu7ypuapQB"
   },
   "outputs": [],
   "source": [
    "#### BEGIN SOLUTION\n",
    "print('Yes we can find all these factors are true or not by using the rt_re to a regex pattern that identifies retweets and hash_link_re to a regex pattern that identifies tweets with hashtags or links.')\n",
    "#### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6SBTwpsapQB"
   },
   "source": [
    "### Task 7.2\n",
    "Using the `df_trump` tweets construct an interesting plot describing a property of the data and discuss what you found below.\n",
    "\n",
    "**Ideas:**\n",
    "\n",
    "1. How has the sentiment changed with length of the tweets?\n",
    "1. Does sentiment affect retweet count?\n",
    "1. Are retweets more negative than regular tweets?\n",
    "1. Are there any spikes in the number of retweets and do the correspond to world events? \n",
    "1. What terms have an especially positive or negative sentiment?\n",
    "\n",
    "You can look at other data sources and even tweets. Do some plots and discuss. You can add more cells here as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqshSSqVapQB"
   },
   "outputs": [],
   "source": [
    "#### BEGIN SOLUTION\n",
    "\n",
    "# In this cell me and my partner created a single plot showing both the distribution of tweet sentiments for tweets containing nytimes , as well as the distribution of tweet sentiments for tweets containing fox\n",
    "sns.distplot(df_trump[trump['text'].str.lower().str.contains(\"nytimes\")]['polarity'],label = 'nytimes')\n",
    "sns.distplot(df_trump[trump['text'].str.lower().str.contains(\"fox\")]['polarity'],label = 'fox')\n",
    "plt.title('Distributions of Tweet Polarities (nytimes vs. fox)')\n",
    "plt.legend()\n",
    "\n",
    "#### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs8GlQj2apQB"
   },
   "source": [
    "#### BEGIN SOLUTION\n",
    "Discussion: \"Enter question you tried answering\"\n",
    "\n",
    "Answer: \n",
    "Byung-Chul Han answered parts 2, 4, 6, and Varun Kumar answered parts 3, 5, 7\n",
    "#### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdfBoczTapQB"
   },
   "source": [
    "### Group Part - Find Something interesting (Optional for Individuals)\n",
    "Is there still something interesting to find in this data set? Use your own imagination to ask some good questions. Don't be bias and look for the answer in data. Don't ask us what we want, because we do not know either. This will be for EXTRA CREDIT for individuals but part of the regular assignment for groups. Add any cells below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting thing we found regarding the data set is the visible and discernible amount of bias and level of control one can have over implicit tone and sentiment even within tweets. For someone of the president's stature, status, and power, to wield the ability to influence the opinions of the masses through something as simple as a one sentence long tweet is an ability that comes with a large level of responsibility. For example, the bias that comes in when mentioning fox news over the new york times is clear evidence of the president's ability to sway public opinion, catering to the president's own personal and political biases.\n",
    "\n",
    "Additionally, some good questions to ask regarding this project would include:\n",
    "1) How did certain Tweets influence the public opinion of the president? Ie was there a causal relationship between tweets and their respective polarities and the president's approval rating?\n",
    "2) How effective was Tweeting during election periods (ie presidential election, midterm elections) as well as during crises (COVID-19, etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHDvFI8bapQB"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Submission Instructions</h2> \n",
    "<b> File Name:</b> Please name the file as yourSection_yourNetID_midsemester.jpynb<br>\n",
    "<b> Group Projects:</b> Each person in the group must submit a copy with both names listed. If you are doing a group project, you must inform your TA prior to 11/3/21 that you intend to work as a group and submit your name and your partner name. We will <b>not accept group work</b> if your TA has not been notified.<br>\n",
    "<b> Submit To: </b> Canvas &rarr; Assignments &rarr; midsemester (remove all output. Do not submit data files<br>\n",
    "<b>Warning:</b> Failure to follow directions may result in loss points.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjF3QW2tapQB"
   },
   "source": [
    "Created by Andy Guna @2019-2021 Credits: Josh Hug, and Berkeley Data Science Group, Steve Skiena, David Rodreguez"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MidSemester Project - F21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
